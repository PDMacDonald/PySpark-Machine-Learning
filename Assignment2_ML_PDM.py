# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19xljy5Yqvv8GgJvIdwiZzpRwLKhU4ebc

**Header and Dependency Setup**
"""

###############################################################################
# IAA 2020 Big Data Session 4 Assignment #2
# @author: Preston MacDonald
# @date: 3-29-
###############################################################################


#Install dependencies
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!rm spark-2.4.5-bin-hadoop2.7.tgz
!wget --no-cookies --no-check-certificate https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz
!tar zxvf spark-2.4.5-bin-hadoop2.7.tgz
!pip install -q findspark
!pip install pyspark

"""**Load Data**"""

#Load dataset
!wget https://raw.githubusercontent.com/zaratsian/Datasets/master/banking_attrition.csv

"""**Import Python / Spark Libraries**"""

#Import Python / Spark Libraries
import os
os.environ["JAVA_HOME"]  = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.5-bin-hadoop2.7"

import datetime, time
import re, random, sys

# Note - Not all of these will be used, but I've added them for your reference as a "getting started"
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, ArrayType, IntegerType, StringType, FloatType, LongType, DateType
from pyspark.sql.functions import struct, array, lit, monotonically_increasing_id, col, expr, when, concat, udf, split, size, lag, count, isnull
from pyspark.sql import Window
from pyspark.ml.linalg import Vectors
from pyspark.ml.regression import GBTRegressor, LinearRegression, GeneralizedLinearRegression, RandomForestRegressor
from pyspark.ml.classification import GBTClassifier, RandomForestClassifier
from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer, IndexToString
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator

#My added imports
from pyspark.ml.stat import Correlation
from pyspark.ml.stat import Summarizer
from pyspark.sql import Row
from pyspark.ml.tuning import CrossValidator
from pyspark.ml.feature import OneHotEncoderEstimator
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from sklearn.metrics import mean_squared_error
from pyspark.ml.classification import RandomForestClassifier

"""**Create Spark Session**"""

#Create spark session
spark = SparkSession.builder.appName("Spark ML Assignment").master("local[*]").getOrCreate()

"""**Load CSV Data into Spark Dataframe**"""

#Load CSV data into Spark Dataframe
rawData = spark.read.load('banking_attrition.csv',
                               format = 'csv',
                               header = True,
                               inferSchema = True)

"""**Data Exploration**"""

####### Using show and groupBys
#Show top 10 rows in dataset
print("Showing top 10 rows:")
rawData.show(n=10)

#group by profession
print("\nCount of obs by profession:")
rawData.groupBy("profession").count().show()

#Average age for balance
print("\nAverage age for each balance:")
rawData.groupBy("balance").avg("age").show()

####### Using correlation and Summarizer

#Select features
features = ["age", "charges", "customer_contacts", "attrition"]

va = VectorAssembler(inputCols= features, outputCol = "features") #Create Vector Assember
featuresData = va.transform(rawData) #transform original dataset to include new col of vectors
featuresData.show(n=2)

#Calculate correlation and display
r1 = Correlation.corr(featuresData, "features", method = 'pearson').head()
print("Pearson correlation matrix:\n" + str(r1[0]))

#Calculate mean statistic for the list of features in order
summarizer = Summarizer.metrics("mean")
featuresData.select(summarizer.summary(featuresData.features)).show(truncate=False)

"""**Split the Spark Dataframe into Train and Test**"""

#Splitting dataframe with randomsplit
splits = rawData.randomSplit(weights= [.7, .3], seed= 12345)

print("training obs count: ", splits[0].count())
print("test obs count: ", splits[1].count())

train = splits[0]
test = splits[1]

"""**Feature Engineering & Define Model**"""

#################################################
#Feature Engineering
#################################################
catFeatures = ["age_group", "profession", "marital_status", "education", 
               "default", "housing", "loan", "gender", "balance", "membership"]
target = "attrition"

#Create copies of train and text to create index versions 
train_indexed = train
test_indexed = test

# Loop through categorical features from list above and create indexed versions
# in both training and test
for feature in catFeatures:
  indexer = StringIndexer(inputCol = feature, outputCol = (feature + "Index"))
  train_indexed = indexer.fit(train_indexed).transform(train_indexed)
  test_indexed = indexer.fit(test_indexed).transform(test_indexed)


#prep target for pipeline
fi = StringIndexer(inputCol= target, outputCol = 'label').fit(train_indexed)

#################################################
#Model Prep & Definition
#################################################

#Create list of all inputs
modelFeatures = ["age", "age_groupIndex", "professionIndex", 
                 "marital_statusIndex", "educationIndex", "defaultIndex", 
                 "housingIndex", "loanIndex", "genderIndex", "balanceIndex", 
                 "membershipIndex", "charges", "customer_contacts"]

#Inputs as vector
va = VectorAssembler(inputCols= modelFeatures, outputCol = "features")


#Define Model
lr = LogisticRegression()
rf = RandomForestClassifier()

#Build Label Converter
labelConverter = IndexToString(inputCol="prediction", 
                               outputCol= "predictionLabel",
                               labels = fi.labels)

"""**Fit/Train ML Model**"""

pipeline = Pipeline(stages=[fi, va, rf])

model = pipeline.fit(train_indexed)

type(model)

"""**Make Predictions on Test Set**"""

pred = model.transform(test_indexed)
pred.show(10,False)

"""**Evaluate Model Against Test Dataframe**"""

############################################################
#Confusion Matrix
############################################################
tp = pred[(pred.label == 1) & (pred.prediction == 1)].count()
tn = pred[(pred.label == 0) & (pred.prediction == 0)].count()
fp = pred[(pred.label == 0) & (pred.prediction == 1)].count()
fn = pred[(pred.label == 1) & (pred.prediction == 0)].count()
print ("True Positives:", tp)
print ("True Negatives:", tn)
print ("False Positives:", fp)
print ("False Negatives:", fn)
print ("Total", pred.count())
print("\n")

############################################################
#Precision and recall
############################################################
print("recall: ", float(tp)/(tp + fn))
print("precision: ", float(tp) / (tp + fp))
print("accuracy: ", (float(tp) + tn) / (pred.count()))
print("\n")
############################################################
#MSE
############################################################
true_target = [int(row.label) for row in pred.select('label').collect()]
pred_target = [int(row.prediction) for row in pred.select('prediction').collect()]

print("MSE: ", mean_squared_error(true_target, pred_target))
print("RMSE: ", mean_squared_error(true_target, pred_target, squared = False))